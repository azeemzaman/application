\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
%\usepackage{wordlike}
\usepackage{setspace}
\usepackage{amssymb}
\doublespacing
%opening
\title{}
\author{}

\begin{document}

%\maketitle

I entered college \emph{certain} that I would be doing a PhD in pure mathematics, so naturally my plans changed.  Being involved in research was a major factor in deciding to continue on to a PhD in statistics.  I got started doing research while an undergraduate at the University of Wisconsin-Madison by taking a course taught by Professor Melanie Wood which was designed to introduce undergraduates to mathematical research.  The topic of interest was the behavior of certain polynomials over finite fields as the degree of the polynomial increased to infinity.  Over an algebraically closed field (or even a non-algebraically closed field like $\mathbb{R}$), the number of roots of a polynomial can be easily understood, but over a finite field, such as $\mathbb{Z}/p\mathbb{Z}$, the integers modulo $p$, the situation is far more complex.  In the class we investigated the distribution of points on smooth superelliptic curves over a fixed finite field as the degree of the curves increased to infinity.  We studied these objects using \texttt{Sage}, a mathematics software system implemented in \texttt{Python} by simulating polynomials with random coefficients from $\mathbb{Z}/p\mathbb{Z}$ and testing the elements of $\mathbb{Z}/p\mathbb{Z}$ to find the roots of the polynomials.  I found this work so enjoyable that I spent my spring break learning to implement multiprocessing, with which I managed to increase the speed of the code and annoy the other students attempting to run code on the server.  In fact, I enjoyed the work so much that I continued working with Professor Wood during the summer and helped to contribute to a paper that was published in \emph{Proceedings of the American Mathematical Society}.  


My carefully laid plan, however, was destined to go awry.  Although I still love mathematics, I found that my interests were much broader.  In addition to working my way to graduate level classes in abstract algebra and mathematical logic, I devoted time to other subjects as well.  I studied Russian, culminating in a year-long study abroad program in Almaty, Kazakhstan, for which I was fully funded by various sources, including the highly competitive Boren Scholarship.  I was among the best students, having achieved the Superior level on the ACTFL scale after the first semester.  While in Kazakhstan I took a course in mathematical economics with the local students, briefly interned at the tax bureau (which was just as Kafkaesque as you would imagine), wrote a paper on the evolution of the American tax system, and kept the number of times I embarrassed myself in the dozens.  Together with my study of Russian I also studied economics, which became my second major.  I decided to complete a joint master's degree in statistics and economics, as I was unsure whether I wanted to continue on to a PhD or leave for work in the federal government.  Once I started doing research again as a graduate student, the decision was made.    


One of my first experiences was with the Spectrum Lab in the economics department, run by Professor Michelle Connolly.  The lab was formed to study the auctions held by the Federal Communications Commission to allocate radio spectrum licenses for cellular usage.  The auctions are of interest for a variety of reasons, not the least of which is the immense value of the spectrum and the vast sums of money spent to acquire it.  Although many papers have been written about these auctions, they often omit potentially important covariates or fail to fully utilize the available data.  For example, when bidding on spectrum, a provider will take into consideration the existing infrastucture in the area, as higher levels of infrastructure will reduce the financial outlay necessary to profit from the license.  To account for this, we collected data on the internet speeds available in the area covered by each license, which should be a good proxy for cellular infrastructure.  On a more sophisticated level, we are shifting from modeling bid amounts to modeling valuations, which represent the maximum amount a firm would pay for a license.  The change is important because a bid tells us about the strategy a firm is using during the auction, whereas the valuation is what actually drives behavior.  To accomplish this I noticed that the structure of the auction reveals the underlying valuations.  During the auction the bidders take turns increasing their bids, so we can use range from the bidder's highest bid to the first bid that surpassed it as a highly credible interval in which we can impute the valuation.  Using this approach greatly increases the amount of available data.

%Our goal was to understand the influence of various factors on the price of the spectrum.  The statistical analysis itself was fairly straight-forward, but I learned a lot about the collaboration of statisticians wit other disciplines, and developed the ability to work with messy data.  My data-wrangling skills definitely improved as a result of this research, as I was tasked with many tedious data cleaning tasks, which provided valuable experience.  The research was the first time I had to consider the interpretation of a result and about the common issues associated with regression, such as the adverse effects of multicollinearity.  The research also taught me about the importance and power of larger research groups.  The project involved a combination of data analysis and close reading of long and tedious government documents to understand the continuously evolving rules of the auctions.  The goal of understanding these auctions involved controlling for many possible significant factors.  For example, over time the efficiency of wireless data transmission has increased, which would tend to reduce the amount of spectrum that a cellular company, such as Verizon or AT\& T would need to satisfy the demand of their customers.  On the other hand, the number of consumers using data intensive applications has increased, which may force cellular companies to purchase more spectrum to satisfy the growing demand.  This is one of many challenges we encountered in our research.  The research also combined theoretical results with their empirical implementations.  Auction theory and design are well studied in theoretical contexts and we were able to incorporate these ideas into our modeling.  

  I am fortunate to be involved in the Neural Basis of Perception Laboratory, led by Professors Jennifer Groh from Neuroscience and Surya Todkar from statistics.  The goal of the research was to understand the firing rate of neurons in a monkey's brain as it is exposed to two simultaneous auditory stimuli from different locations.  In the experiment, a monkey listens to one sound coming from the left, followed by a sound with a different frequency from the right, and then both sounds simultaneously.  While the monkey is listening to these sounds, an electrode is measuring the electrical impulses being send by a single neuron.  When I joined the group I began working with a Bayesian model originally developed by Chris Glynn in his PhD thesis in collaboration with Professor Tokdar.  The firing rate  of the neuron as measured by an electrode is modeled by a non-homogeneous Poisson Process.  The goal is to model the rate of neuron when it is exposed to both sounds as a Poisson process whose rate is some type of weighted average of the firing rates for the single sounds.   The weight $\alpha$ various over the duration of the sounds as a Gaussian process that represents how similar to sound A or B the firing rate of the dual sound is.  The behavior of this weight parameter is interesting because it tells us how the neuron is reacting to two stimuli, such as whether the neuron is providing equal attention to both sounds (when $\alpha \approx 0.5$) or giving preference to one of the sounds (when $alpha$ is near 0 or 1).  My initial role in the project was maintaining code and helping to develop plots to visualize the results of the MCMC.  I also worked to optimize certain matrix operations using an Cholesky decomposition to improve the speed of the update for the Polya-Gamma parameters, which was the bottleneck in the complex sampler.  This paper is currently in preparation for submission.  
  
  My research interests were shaped by my work in the Neural Lab.  In particular, I am interested in hierarchical Bayesian modeling and the use of Markov Chain Monte Carlo to fit these models.  For instance, my current research with the Neural Lab involves an extension of the current model.  Instead of fixing the length scale of the Gaussian process $\alpha$, the model attempts to learn the distribution of length scales, which practically correspond to different types of behavior for the neuron; a large length scale reflects a neuron whose behavior is relatively constant, while a short length scale shows that the neuron may be ``switching'' between the two different stimuli.  As a result of inadequate sample sizes, we found that the length scale was poorly learned in the sense that the uniform prior on the discrete fixed length scales was essentially the same as the posterior distribution conditional on the observed data.  To rectify this situation, I am working on a hierarchical extension to the model so that the distribution of the length scales can be learned jointly across multiple trials.  To achieve this, I employ a conjugate multinomial-Polya gamma model with stick-breaking to sample correlated probability vectors.  The approach would be useful in the case where there are two mixture distributions with similar weights, but not enough data to estimate the weights accurately, as we can share information between the two mixtures.



\end{document}
